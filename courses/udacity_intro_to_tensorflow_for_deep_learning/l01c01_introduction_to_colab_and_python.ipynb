{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "lst = [1,2,2,2,3,4,4,4,5,6,7]\n",
        "dup = {}\n",
        "\n",
        "for item in lst:\n",
        "  if item in dup:\n",
        "    dup[item] += 1\n",
        "  else:\n",
        "    dup[item] = 1\n",
        "for key in dup:\n",
        "  # if dup[key] > 1:\n",
        "    print(key, 'is',dup[key] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbMdM5ejyRks",
        "outputId": "d585fe5e-71ad-484e-eeae-ebb7e7d4affa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 is 1\n",
            "2 is 3\n",
            "3 is 1\n",
            "4 is 3\n",
            "5 is 1\n",
            "6 is 1\n",
            "7 is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['qqwsw','eerdece','e','frgfrg']\n",
        "resut = ''\n",
        "for word in words:\n",
        "  resut += word + str(len(word))\n",
        "print(resut)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O41yYjOh50ix",
        "outputId": "273a9315-bc53-4865-dd9a-109b627c95d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qqwsw5eerdece7e1frgfrg6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 121\n",
        "strnum = str(num)\n",
        "trnum = strnum[::-1]\n",
        "\n",
        "if trnum == strnum:\n",
        "  print('yes')\n",
        "else:\n",
        "  print('no')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwJaNMQI6Sfg",
        "outputId": "8977e4b5-549f-4d4f-a3ea-1d97f113e5fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 121\n",
        "strnum = str(num)\n",
        "revnum = strnum[::-1]\n",
        "\n",
        "if strnum == revnum:\n",
        "    print('yes')\n",
        "else:\n",
        "    print('no')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhfTjgOA6o8t",
        "outputId": "8fd7fe84-f8d4-4243-e0c4-9ea46bec3192"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "# 1. Sample Data\n",
        "data = [\n",
        "    (\"U001\", \"2024-01-01 10:00:00\", 100),\n",
        "    (\"U001\", \"2024-02-01 09:00:00\", 105),\n",
        "    (\"U002\", \"2024-01-05 11:00:00\", 200),\n",
        "    (\"U002\", \"2024-01-05 08:00:00\", 195),\n",
        "]\n",
        "columns = [\"user_id\", \"last_updated\", \"value\"]\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.createDataFrame(data, columns)\n",
        "# 2. Convert to timestamp if needed\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "df = df.withColumn(\"last_updated\", to_timestamp(\"last_updated\"))\n",
        "# 3. Define Window Spec\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"last_updated\").desc())\n",
        "# 4. Assign Row Numbers\n",
        "ranked_df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "# 5. Filter for Most Recent Record\n",
        "final_df = ranked_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUrckDV964Je",
        "outputId": "538bcb6d-3ac5-4f9b-ba22-84369b4b280f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-----+\n",
            "|user_id|       last_updated|value|\n",
            "+-------+-------------------+-----+\n",
            "|   U001|2024-02-01 09:00:00|  105|\n",
            "|   U002|2024-01-05 11:00:00|  200|\n",
            "+-------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, row_number, sum, rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"PySpark_Interview_Demo\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame 1: Users\n",
        "users_data = [\n",
        "    (1, \"Alice\", \"2024-01-01\", \"HR\"),\n",
        "    (2, \"Bob\", \"2024-02-01\", \"Finance\"),\n",
        "    (3, \"Charlie\", \"2024-01-15\", \"HR\"),\n",
        "    (4, \"David\", \"2024-03-01\", \"IT\"),\n",
        "    (5, \"Eve\", \"2024-01-10\", \"Finance\"),\n",
        "]\n",
        "users_cols = [\"user_id\", \"name\", \"join_date\", \"department\"]\n",
        "\n",
        "users_df = spark.createDataFrame(users_data, users_cols)\n",
        "\n",
        "# Sample DataFrame 2: Salaries\n",
        "salary_data = [\n",
        "    (1, 1000),\n",
        "    (2, 2000),\n",
        "    (3, 1500),\n",
        "    (4, 2200),\n",
        "    (6, 1800)  # user_id 6 does not exist in users_df\n",
        "]\n",
        "salary_cols = [\"user_id\", \"salary\"]\n",
        "salary_df = spark.createDataFrame(salary_data, salary_cols)"
      ],
      "metadata": {
        "id": "57lBoRIH8oeR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Join\n",
        "inner_join = users_df.join(salary_df, \"user_id\", \"inner\")\n",
        "# Left Join\n",
        "left_join = users_df.join(salary_df, \"user_id\", \"left\")\n",
        "# Left Anti Join (users not in salary)\n",
        "left_anti = users_df.join(salary_df, \"user_id\", \"left_anti\")\n",
        "# Left Semi Join (users with matching salary)\n",
        "left_semi = users_df.join(salary_df, \"user_id\", \"left_semi\")"
      ],
      "metadata": {
        "id": "l0XLgMKuV7ss"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate users for merge simulation\n",
        "extra_users = [\n",
        "    (6, \"Frank\", \"2024-04-01\", \"IT\"),\n",
        "    (7, \"Grace\", \"2024-04-10\", \"HR\")\n",
        "]\n",
        "extra_users_df = spark.createDataFrame(extra_users, users_cols)\n",
        "\n",
        "# Combine\n",
        "merged_users = users_df.union(extra_users_df).dropDuplicates()\n"
      ],
      "metadata": {
        "id": "xyGArEvaYQEo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join salary with users first\n",
        "user_salary_df = users_df.join(salary_df, \"user_id\", \"left\")\n",
        "\n",
        "# Add join_date as timestamp\n",
        "user_salary_df = user_salary_df.withColumn(\"join_date\", col(\"join_date\").cast(\"timestamp\"))\n",
        "\n",
        "# Define window by department\n",
        "window_spec = Window.partitionBy(\"department\").orderBy(col(\"join_date\"))\n",
        "\n",
        "# Add running sum\n",
        "user_salary_df = user_salary_df.withColumn(\"running_total\",\n",
        "    sum(\"salary\").over(window_spec)\n",
        ")\n",
        "\n",
        "# Add row number\n",
        "user_salary_df = user_salary_df.withColumn(\"row_num\",\n",
        "    row_number().over(window_spec)\n",
        ")\n"
      ],
      "metadata": {
        "id": "vKt7T1UoYTXF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter users who joined after Jan 15\n",
        "filtered_df = users_df.filter(col(\"join_date\") > \"2024-01-15\")\n",
        "\n",
        "# Group by department and count\n",
        "dept_count = users_df.groupBy(\"department\").count()\n",
        "\n",
        "# Sort by name\n",
        "sorted_users = users_df.orderBy(\"name\")\n"
      ],
      "metadata": {
        "id": "-Lh_Bo8ZYVXw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_join.show()\n",
        "user_salary_df.select(\"user_id\", \"name\", \"department\", \"salary\", \"running_total\", \"row_num\").show()\n",
        "dept_count.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCfJ_W_PYWdT",
        "outputId": "a254368d-a448-4377-d551-dc07f0c2b4d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----------+----------+------+\n",
            "|user_id|   name| join_date|department|salary|\n",
            "+-------+-------+----------+----------+------+\n",
            "|      1|  Alice|2024-01-01|        HR|  1000|\n",
            "|      2|    Bob|2024-02-01|   Finance|  2000|\n",
            "|      3|Charlie|2024-01-15|        HR|  1500|\n",
            "|      4|  David|2024-03-01|        IT|  2200|\n",
            "+-------+-------+----------+----------+------+\n",
            "\n",
            "+-------+-------+----------+------+-------------+-------+\n",
            "|user_id|   name|department|salary|running_total|row_num|\n",
            "+-------+-------+----------+------+-------------+-------+\n",
            "|      5|    Eve|   Finance|  NULL|         NULL|      1|\n",
            "|      2|    Bob|   Finance|  2000|         2000|      2|\n",
            "|      1|  Alice|        HR|  1000|         1000|      1|\n",
            "|      3|Charlie|        HR|  1500|         2500|      2|\n",
            "|      4|  David|        IT|  2200|         2200|      1|\n",
            "+-------+-------+----------+------+-------------+-------+\n",
            "\n",
            "+----------+-----+\n",
            "|department|count|\n",
            "+----------+-----+\n",
            "|        HR|    2|\n",
            "|   Finance|    2|\n",
            "|        IT|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, \"2024-01-01 09:00:00\", \"HR\"),\n",
        "    (1, \"2024-01-03 10:00:00\", \"HR\"),  # latest for user 1\n",
        "    (2, \"2024-01-02 14:00:00\", \"IT\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"user_id\", \"last_updated\", \"dept\"])\n",
        "\n",
        "# Convert to timestamp\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "df = df.withColumn(\"last_updated\", to_timestamp(\"last_updated\"))\n",
        "\n",
        "# Define Window\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(df[\"last_updated\"].desc())\n",
        "\n",
        "# Apply row_number and filter\n",
        "from pyspark.sql.functions import col\n",
        "df_ranked = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "most_recent = df_ranked.filter(col(\"row_num\") == 1).drop(\"row_num\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrUxk7D8YXo2",
        "outputId": "a79b0493-aa47-4e3c-d3f3-d841ca92f33c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----+\n",
            "|user_id|       last_updated|dept|\n",
            "+-------+-------------------+----+\n",
            "|      1|2024-01-03 10:00:00|  HR|\n",
            "|      2|2024-01-02 14:00:00|  IT|\n",
            "+-------+-------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# large_df: transaction data, millions of rows\n",
        "# small_df: lookup table (e.g. department names), ~10 rows\n",
        "\n",
        "joined_df = large_df.join(broadcast(small_df), on=\"dept_id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "xslC6JfyZqUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Alice\", \"HR\", 1000),\n",
        "    (\"Bob\", \"HR\", 1200),\n",
        "    (\"Charlie\", \"HR\", 1200),\n",
        "    (\"David\", \"IT\", 1500),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"name\", \"dept\", \"salary\"])\n",
        "\n",
        "# Define window partitioned by department and ordered by salary descending\n",
        "window_spec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Add rank column\n",
        "df_with_rank = df.withColumn(\"rank\", rank().over(window_spec)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfeQC7YrcIc-",
        "outputId": "b746e390-8910-4508-d2cd-f18e3e232fd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------+----+\n",
            "|   name|dept|salary|rank|\n",
            "+-------+----+------+----+\n",
            "|    Bob|  HR|  1200|   1|\n",
            "|Charlie|  HR|  1200|   1|\n",
            "|  Alice|  HR|  1000|   3|\n",
            "|  David|  IT|  1500|   1|\n",
            "+-------+----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number, rank, to_timestamp\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"LatestUserRecords\").getOrCreate()\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"login\", \"2024-05-20 10:00:00\"),\n",
        "    (1, \"login\", \"2024-05-21 09:00:00\"),\n",
        "    (2, \"logout\", \"2024-05-19 18:00:00\"),\n",
        "    (2, \"logout\", \"2024-05-21 20:00:00\")\n",
        "]\n",
        "columns = [\"id\", \"act\", \"time\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df = df.withColumn('time', to_timestamp('time'))\n",
        "wf = Window.partitionBy('id').orderBy(col('time').desc())\n",
        "df1 = df.withColumn('rn', row_number().over(wf))\n",
        "dfdf = df1.filter(col('rn') == 1).drop('rn')\n",
        "dfdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CtU-4ancLMg",
        "outputId": "78e26fe8-61d2-4a9c-e825-5b6148494916"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-------------------+\n",
            "| id|   act|               time|\n",
            "+---+------+-------------------+\n",
            "|  1| login|2024-05-21 09:00:00|\n",
            "|  2|logout|2024-05-21 20:00:00|\n",
            "+---+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5iygzi1Mhab"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}